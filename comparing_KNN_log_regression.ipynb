{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3- Rosie Faccone\n",
    "\n",
    "In this project, we performed classification on the admissions data we've been working with in projects 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   admit    gre   gpa  prestige\n",
      "0      0  380.0  3.61       3.0\n",
      "1      1  660.0  3.67       3.0\n",
      "2      1  800.0  4.00       1.0\n",
      "3      1  640.0  3.19       4.0\n",
      "4      0  520.0  2.93       4.0\n"
     ]
    }
   ],
   "source": [
    "# load the admissions.csv file found in ../../project-2/assets/\n",
    "df = pd.read_csv(\"../../project-2/assets/admissions.csv\")\n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.00000</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>588.040201</td>\n",
       "      <td>3.39093</td>\n",
       "      <td>2.486216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.628513</td>\n",
       "      <td>0.38063</td>\n",
       "      <td>0.945333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.26000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.13000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.39500</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.67000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre        gpa    prestige\n",
       "count  400.000000  398.000000  398.00000  399.000000\n",
       "mean     0.317500  588.040201    3.39093    2.486216\n",
       "std      0.466087  115.628513    0.38063    0.945333\n",
       "min      0.000000  220.000000    2.26000    1.000000\n",
       "25%      0.000000  520.000000    3.13000    2.000000\n",
       "50%      0.000000  580.000000    3.39500    2.000000\n",
       "75%      1.000000  660.000000    3.67000    3.000000\n",
       "max      1.000000  800.000000    4.00000    4.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use shape and describe to see if any rows are missing values\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in any missing values for gre or gpa with the means of those columns respectively\n",
    "df.gre.fillna(df.gre.mean(), inplace=True)\n",
    "df.gpa.fillna(df.gpa.mean(),  inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    150\n",
       "3.0    121\n",
       "4.0     67\n",
       "1.0     61\n",
       "Name: prestige, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill in missing values for prestige with value which occurs the most\n",
    "# hint: use .value_counts()\n",
    "df.prestige.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.prestige.fillna(2.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change dtpyes of gre and prestige to int and gpa to float\n",
    "df.gre = df.gre.astype(int)\n",
    "df.gpa = df.gpa.astype(float)\n",
    "df.prestige = df.prestige.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>588.040000</td>\n",
       "      <td>3.390930</td>\n",
       "      <td>2.48500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.338353</td>\n",
       "      <td>0.379675</td>\n",
       "      <td>0.94446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.390930</td>\n",
       "      <td>2.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa   prestige\n",
       "count  400.000000  400.000000  400.000000  400.00000\n",
       "mean     0.317500  588.040000    3.390930    2.48500\n",
       "std      0.466087  115.338353    0.379675    0.94446\n",
       "min      0.000000  220.000000    2.260000    1.00000\n",
       "25%      0.000000  520.000000    3.130000    2.00000\n",
       "50%      0.000000  580.000000    3.390930    2.00000\n",
       "75%      1.000000  660.000000    3.670000    3.00000\n",
       "max      1.000000  800.000000    4.000000    4.00000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use count again to check that you have no missing values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prestige_1</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prestige_1  prestige_2  prestige_3\n",
       "0         0.0         0.0         1.0\n",
       "1         0.0         0.0         1.0\n",
       "2         1.0         0.0         0.0\n",
       "3         0.0         0.0         0.0\n",
       "4         0.0         0.0         0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and add dummy variables for prestige using the prefix 'prestige'\n",
    "# drop the prestige_4 column (Recall: why are we dropping this column?)\n",
    "dum= pd.get_dummies(df.prestige, prefix='prestige')\n",
    "dum.drop('prestige_4', axis=1, inplace=True)\n",
    "dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>prestige_1</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  prestige  prestige_1  prestige_2  prestige_3\n",
       "0      0  380  3.61         3         0.0         0.0         1.0\n",
       "1      1  660  3.67         3         0.0         0.0         1.0\n",
       "2      1  800  4.00         1         1.0         0.0         0.0\n",
       "3      1  640  3.19         4         0.0         0.0         0.0\n",
       "4      0  520  2.93         4         0.0         0.0         0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df.join(dum)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use these features for classification\n",
    "X = df[['gre','gpa','prestige_1','prestige_2','prestige_3']]\n",
    "y=df.admit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6925\n"
     ]
    }
   ],
   "source": [
    "# using k-fold cross-validation with k = 10\n",
    "# calculate mean test set accuracy of LogisticRegression using all features to predict admit\n",
    "kf = KFold(len(df), n_folds= 10, shuffle=True)\n",
    "scores=[]\n",
    "for train_idx, test_idx in kf:\n",
    "    model=LogisticRegression()\n",
    "    model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "    score= model.score(X.iloc[test_idx], y.iloc[test_idx])\n",
    "    scores.append(score)\n",
    "\n",
    "print np.mean(scores)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.65750, std: 0.08591, params: {'n_neighbors': 3},\n",
       " mean: 0.67000, std: 0.07228, params: {'n_neighbors': 5},\n",
       " mean: 0.68000, std: 0.07730, params: {'n_neighbors': 7},\n",
       " mean: 0.68500, std: 0.07176, params: {'n_neighbors': 9},\n",
       " mean: 0.67250, std: 0.08548, params: {'n_neighbors': 11},\n",
       " mean: 0.69250, std: 0.08370, params: {'n_neighbors': 13},\n",
       " mean: 0.67500, std: 0.08139, params: {'n_neighbors': 15},\n",
       " mean: 0.67500, std: 0.07984, params: {'n_neighbors': 17},\n",
       " mean: 0.66750, std: 0.08807, params: {'n_neighbors': 19},\n",
       " mean: 0.66500, std: 0.09028, params: {'n_neighbors': 21},\n",
       " mean: 0.67500, std: 0.08588, params: {'n_neighbors': 23},\n",
       " mean: 0.67750, std: 0.07862, params: {'n_neighbors': 25},\n",
       " mean: 0.67500, std: 0.08292, params: {'n_neighbors': 27},\n",
       " mean: 0.68250, std: 0.08066, params: {'n_neighbors': 29},\n",
       " mean: 0.68000, std: 0.08124, params: {'n_neighbors': 31},\n",
       " mean: 0.67750, std: 0.08548, params: {'n_neighbors': 33},\n",
       " mean: 0.67000, std: 0.07566, params: {'n_neighbors': 35},\n",
       " mean: 0.67250, std: 0.07941, params: {'n_neighbors': 37},\n",
       " mean: 0.68000, std: 0.07969, params: {'n_neighbors': 39},\n",
       " mean: 0.68000, std: 0.07969, params: {'n_neighbors': 41},\n",
       " mean: 0.67750, std: 0.08250, params: {'n_neighbors': 43},\n",
       " mean: 0.68000, std: 0.08047, params: {'n_neighbors': 45},\n",
       " mean: 0.68750, std: 0.07846, params: {'n_neighbors': 47},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 49},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 51},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 53},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 55},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 57},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 59},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 61},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 63},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 65},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 67},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 69},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 71},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 73},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 75},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 77},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 79},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 81},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 83},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 85},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 87},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 89},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 91},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 93},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 95},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 97},\n",
       " mean: 0.68250, std: 0.07910, params: {'n_neighbors': 99}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use grid search to select the best fit k for k Nearest Neighbor\n",
    "# use kNN with neighbors weighted by distance (weights='distance')\n",
    "# search through a grid from k = 3 to 99 at intervals of 2\n",
    "# again use 10-fold crossvalidation\n",
    "# hint: use GridSearchCV\n",
    "\n",
    "parameter = range(3, 100, 2)\n",
    "params = {'n_neighbors': parameter }\n",
    "kf = KFold(len(df), n_folds= 10, shuffle=True)\n",
    "gs = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_grid=params,\n",
    "    cv=kf,\n",
    ")\n",
    "gs.fit(X, y)\n",
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 13}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out best parameter setting found\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6925"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the best score\n",
    "gs.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain both classifiers (logistic regression and kNN using best parameter found) on the full dataset\n",
    "lr =LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "knn= KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for the entire set of observations, get the probablity of class 1 (P(y=1 | x)) for both models\n",
    "prob_lr= lr.predict_proba(X)[:,1]\n",
    "prob_knn= knn.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.669608606616\n",
      "0.720097487814\n"
     ]
    }
   ],
   "source": [
    "# print AUC for both classifiers\n",
    "print metrics.roc_auc_score(y, prob_lr)\n",
    "\n",
    "print metrics.roc_auc_score(y, prob_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4FFW+//H3IezIHkUBAUX2EUXZFQmCEHSAILIqyqIi\nCqOOXmG48hP1jo6oo1eR8aKgOIhBAQMIiqBE9kVJQJYgCIOQQFhlh2zn90fFkJU0SXdXp/N5PU8e\nuqtOV32pJ/nm5JxT3zLWWkREJDiVcDsAERHxHSV5EZEgpiQvIhLElORFRIKYkryISBBTkhcRCWL5\nJnljzFRjTKIxZvMl2rxjjNlpjIk1xtzs3RBFRKSgPOnJfwR0y2unMaY7UN9a2wAYAbzvpdhERKSQ\n8k3y1tqVwPFLNOkFfJLedh1Q2RhTwzvhiYhIYXhjTL4WsC/T+/j0bSIi4jJNvIqIBLGSXjhGPHBt\npve107flYIxRoRwRkQKw1pqCfM7TJG/Sv3IzH3gCmGWMaQv8bq1NzOtAKojmmDBhAhMmTHA7jICg\na3GRrsVF3roWqWmpHD9/nCNnj2T5Onr2qPP6XM7tJy+cpGq5qoSWDyW0fCjVy1UntHwon00NpUxq\nKCWTQgm5EErIheqUuBBKmZRQlnxVmXp1fTM4YkyB8jvgQZI3xswEwoDqxpjfgBeA0oC11k6x1i4y\nxtxtjNkFnAGGFjgaEZHLcOjMIZb8uoSEUwkXE3W2pH3i/Akql62ckbBDy4cSWi6U6uWrc2WFK2ly\nZZMcybxK2SqElAjJOE9KCnz2GcxcAr/GQ9WqLv6nL1O+Sd5aO8iDNqO8E46IyKX9euxXouKiiNoR\nxebEzXS+rjP1q9anevnqNKjeIGsyLx9K1bJVsyTsgti5E0aNgocfhooVvfQf8RNvjMlLAYSFhbkd\nQsDQtbhI1+KiP66FtZaYgzFExUXxZdyXHDpziF6NevG32//GndfdSdmSZX0WQ1QUfPkl/P471KwJ\n77zjs1P5jPHnGLkxxmpMXkTyk5yazIrfVjg99rgoypQsQ+/GvYloHEGbWm0K3TP31PDhYC107Aj1\n6jn/usEYU+CJVyV5EQkIZ5LOsPjXxUTFRbFw50LqV61PROMIIhpH0CS0SaEmHy+lfXs4cCD3fYcP\nw/vvwwMP+OTUHlOSF5Ei6fCZwyz4ZQFRcVFE/yeatrXbEtE4gp6NelK7Um2fnTc1FRISnNc33ACb\nNkGZMrm3rVMHQvzzh0OelORFpMjYfXx3xjDM5sTNdK3flYjGEdzd4G6qlK3ilxg+/BCeespZJVO5\nMsTGQskAnqFUkheRgJV54jQqLorEM4n0bNiTiMYRdL6+s9cmTg8cgO+/96ztt986q2QmTfLKqX2u\nMEk+gH93iUhRlXnidN6OeZQqUYrejXvzr3v+RdvabX0ycTpjBkybBrfc4ln7bnnW1g0u6smLiFec\nSTrDt79+S9SOKL765Suur3o9EY2cidOmVzYt8MRpQgK8/DKkpV26XWyss/pl4sQCnSagqScvIq44\nfOYwX/3yFVE7oli2ZxltarcholEE/9Ppf7i28rX5H8ADW7dCdLQzhn4pt9wCnTp55ZRBRT15Ebks\nu4/vZl7cPKJ2RBF7MNaZOG3kTJxWLVfw+/2XLIG//CXn9tOnoXlzWLiwEEEXcerJi4jPWGuJPRib\nUUrg4OmD9GzYk+faP+fVidPdu+HGG+HFF3Puq6HHEBWYkryI5JCSlsKKvSsyEvsfE6eT757ss4lT\ncJY0Nmnik0MXW0ryIgJknThd+MtC6lWpR0TjCBYOWkizK5v57I5T8S0leZFiLPvEaetarYlo7N2J\nU3GXkrxIMbPn+J6MYZjYg7Hcdf1d9G3al497fVyoidOCmjwZduyAn3+GBg38fvqgp9U1IkHOWsum\nxE0Zd5wmnEqgZ6P0O06v60y5UuVcja9BA+jfH6680lnnfvPNroYTkFTWQESySElLYeVvKzMSe0iJ\nkIxSve1qt/Nbqd6vv4Zx4y7dZts2pydfr55fQiqStIRSRDibfNaZOI1z7jj9Y+L0q0Ff+X3i9MIF\n5ys2Fpo2hWefzbtt6dJQt67fQit2lORFirAjZ484E6dxUXy/5/uMidOXOr1Encp1XIureXOIj4cS\nJWD8eGjRwrVQij0leZEiJv5kPF9s+4KouChiDsZw1/V3cV/T+5jWaxrVylXz+fmPHIG1ay/d5vBh\nZwimVi2fhyP5UJIXKQKSUpNYsGMB02KnsWbfGiIaR/BMu2focn0Xv0+cfvih89W4cd5t7rwTqvn+\n9414QBOvIgFs66GtTI2ZyozNM2h6ZVOGtRhGnyZ9qFC6gtfPdfAgvP12/tUe16yBDh3glVe8HoLk\nQROvIkHkxPkTzNo6i6kxU9l/cj9DbhrC6uGruaHaDT4978aN8OWXzsOrL6VHD7jnHp+GIl6knrxI\nALDWsnzvcqbFTmNe3Dy6XN+FYS2G0bV+V0qW8H5fLCEB+vWDlJSL244fh0aNYP58r59OCkk9eZEi\nKv5kPNM3TWdazDTKlizL8BbDeeOuN7iywpVePU9KStZhmH37nMnR6dOztqvj3oIc8REleRE/yz6J\n2rdpX2b2mUmrmq18spb9+HG4+mrI/kd0587Qtq3XTycBRklexE+2HNrCtJhpGZOow1sM54u+X1C+\nVHmfnfP8eedhHNWrO0M0UvwoyYv40InzJ4jcEsm02Gl+nUT9w6JFMGoU9Onjl9NJANLEq4iX/TGJ\nOjVmKvN3zPf5JGpe1q+/+FDr2bP9dlrxAU28igSA+JPxfBz7MR/FfpQxifpm1ze9PonqqchI5/mo\nzz3nyuklQCjJixTCH5OoU2Omsnb/Wvo16+fTSdTL1bWrc/epFF9K8iIFkNsk6ux+s306iSpSEEry\nIh5yexI1N82awfbtue+zFqZN8288EniU5EUuwVrLD3t/YFrMtIxJ1Bc6vkC3+t389uCN7M6fhx9/\ndF4nJDglfWvUyL1tiRL+i0sCk5K8SC72n9zP9NjpATOJmtncufDkk04JgpYtoUoVJXPJm5ZQiqRL\nSk1i/o75TIuZljGJOqzFMK9Poh4+7JTqLeiPQkwMlC0L//6310KSAOfzJZTGmHDgbaAEMNVa+1q2\n/ZWAGUAdIAR401r7cUECEvG3LYe2MHXjVD79+VO/TKKuWeOMlfftW7DPN2jglCQQ8US+Sd4YUwKY\nBHQGEoANxph51tq4TM2eALZaa3saY0KBHcaYGdbalFwOKeK6PyZRp8ZMJeFUAkNu9u8kapMmqscu\n/uFJT741sNNauxfAGBMJ9AIyJ3kLVEx/XRE4qgQvgSbNpmXcibpgxwK6XN+FF8NepGv9rq5Noor4\nmidJvhawL9P7/TiJP7NJwHxjTAJwBdDfO+GJeMf+k/vp83kfziSdYXiL4fyz6z9dmUR96y0YM8ap\n5S7iD95aXdMNiLHW3mmMqQ8sMcY0t9aezt5wwoQJGa/DwsIICwvzUggiuVu9bzV9v+jL6NajGXPb\nGFfvRE1MhPHjYdw410KQIiA6Opro6GivHCvf1TXGmLbABGttePr7sYDNPPlqjPkKeNVauyr9/XfA\nGGvtj9mOpdU14lcfbvyQcd+N4+OIj7m7wd1uh8PYsc6Sx7Fj3Y5EihJfr67ZANxgjKkLHAAGAAOz\ntdkLdAFWGWNqAA2B3QUJSMQbklOTeXrx0yzdvZQVQ1fQKLSRq/FMnw5HjjiVIbt2dTUUKWY8Wief\nvoTyf7m4hPIfxpgROD36KcaYa4CPgWvSP/KqtfazXI6jnrz43OEzh+n7RV8qlK7AzHtnUrlsZbdD\nomRJGD3auWlp8GC4+Wa3I5KipDA9ed0MJUEl9mAsEZERDLpxEC93ejlgVs2ULOmUIyipe8ylAFRP\nXgSYtWUWo74exaTuk+j/Jy3wEgEleQkCqWmpjF82npk/z2TJ4CXcfLXGQkT+oCQvRdqJ8ye4f+79\nnEo6xYZHNgREAbHMrIW4uILXqREpLNWukyJrx5EdtPmwDfWq1GPp4KUBl+ABYmPhllugfXtVihR3\n6NtOiqRFOxfR4aMOPNPuGSbdPYlSIaXcDimH336DmTPhxhthxQoleXGHhmukSLHWMnHVRN5Z/w5f\n9v+S2+rc5nZIeZo7FxYvdmq/i7hFSV6KjLPJZxk+fzi7ju1i3cPrqF2pttshZXH2LDz2GFy44Lzf\nscMpCTx8uLtxSfGmJC9Fwt7f99J7Vm+aXdWM5UOWU65UObdDyuHIEVi4ECZPvritTRv34hEB3Qwl\nRcDyvcvpP7s//9X+v3i67dOuFhgD2LoVunWD1NSs21NToWpVpwcv4k26GUqCkrWW9398nwk/TGBG\n7xncVf8uF2JwJlCTki5ui4mBa691xtyzu+IK/8Um4gkleQlISalJjF40mpX7VrJq2Cq/PbEpuz17\nnAdm16uXdXv37nDNNbl+RCSgKMlLwEk8nUifz/sQWj6UtcPXUrFMxfw/5GVbt8LatXDgANSv79zQ\nJFIUaeWuBJQfE36k1Qet6HxdZ+b2n+tKggfnCU4ffeT05B991JUQRLxCPXkJGJ9u/pSnFz/N+39+\nn3ub3OuXc37wAaxcmXP76tXOY/oeftgvYYj4jFbXiOtS01IZu3Qsc+PmEtU/ihtr3Oi3c3fuDLfe\nCs2a5dx3zz0QGuq3UETypHryUmQdP3ecAXMGkJqWyqz7ZlG9fHWfn3P1ahgyxFkxc/AgLF0Kt9/u\n89OKFJiSvBRJ2w5vo1dkL3o07MHEuyZSsoR3Rw+thf37s1aAvHDBWeP+0ktOYg8Jgdq1weWl9yKX\npHXyUuTM3zGf4fOH88Zdb/DQzQ/55ByLFsF998GV2YpT9u0LDzzgk1OKBBz15MWv0mwaf1/+d6Zs\nnMKcfnNoXau1V4+/cCEcP+68Xr8e4uNhzhyvnkLE79STlyLhdNJphkQNIf5UPOsfXs81Fb17N5G1\n8Oc/w/33X9wWEeHVU4gUOerJi1/sOb6HXpG9aFWzFZPvmUyZkmW8fg5rnZrt+haTYFOYnrxuhhKf\n+273d7Sb2o5Hb32UD3t+6JMELyK503CN+Iy1lnfXv8srK17hsz6f0em6Tl4/xwsvwKxZf5wPSpf2\n+ilEijQlefGJ8ynnGblwJBsPbGTN8DVcV/W6Qh3v7Fk4fTrn9g0b4Ikn4K70ApWqAimSlZK8eF3C\nqQTunXUv11a+ltXDVlOhdIVCH7NLF9i+HUple5SrMfD889C4caFPIRKUlOTFq9btX0efz/swsuVI\nxnUY57UHfJw9C8uWwc03e+VwIsWGkrx4zcexH/PckueY2nMqPRr1cDscEUFJXrwgJS2FZ799lkU7\nF/HDkB9ocmUTt0MSkXRK8lIoR88epd/sfpQOKc36R9ZTpWwVt0MSkUyU5KXAfk78mV6RvejbtC+v\ndH6FkBIhHn3u3DmnxO/Zs56fa8eOnJOuIpI/JXkpkDnb5vDYwsd4J/wdBt448LI+e+YMbNsG0dGe\nf6ZUKWja9PJiFBEleblMaTaNCdETmL5pOt/c/w231ry1QMcpWVIrZUT8QUlePHbywkkGfzmYY+eO\nseGRDVxV4SqPP7ttG+ze7bw+ccJHAYpIDkry4pFdx3bRK7IXd9S5gy/6fkHpkMurH/D4484DO6qn\nP/hpwAAfBCkiOSjJS74W71rMg1EP8lLYS4xoOeKyPz9xIuzaBTNnwh13+CBAEcmTR1UojTHhxpg4\nY8wvxpgxebQJM8bEGGO2GGOWeTdMcYO1ljdXv8nQeUOZ3Xd2gRI8wLhx8OSTGoMXcUO+9eSNMSWA\nX4DOQAKwARhgrY3L1KYysBroaq2NN8aEWmuP5HIs1ZMvIs4ln+PRrx5l2+FtfNn/S+pUrlPgY5Us\nCefPO/+KyOXzdT351sBOa+1ea20yEAn0ytZmEDDHWhsPkFuCl6Jj34l9dPioA2k2jRVDVxQqwScl\n6SEeIm7yJMnXAvZler8/fVtmDYFqxphlxpgNxpjB3gpQ/GvVb6to82Eb+jfrz4zeMyhfqnyBj7V9\nO5QvD6GhzhObRMT/vPUHdEngFuBOoAKwxhizxlq7y0vHFz/44KcPeH7Z80yPmE74DeGX9VlrYeVK\nZ1jmD3FxcMstzgO1RcQdniT5eCDz3+u107dlth84Yq09D5w3xiwHbgJyJPkJEyZkvA4LCyMsLOzy\nIhavS05N5qlvnuL7/3zPiqEraFi94WUfIz7eeXBHhw5Zt4df3u8KEQGio6OJvpxbwi/Bk4nXEGAH\nzsTrAWA9MNBauz1Tm8bAu0A4UAZYB/S31m7LdixNvAaYQ2cO0feLvlQuU5kZ986gUplKBTrOb7/B\n7bc7/4qId/l04tVamwqMAr4FtgKR1trtxpgRxphH09vEAYuBzcBaYEr2BC+BJ+ZADK0/aM0dde4g\nakBUgRO8iASufHvyXj2ZevIBY9aWWYz+ejST75nMfU3vK/Tx1JMX8Z3C9OS1crmYSU1L5fnvn2fW\n1lksGbyEm66+ye2QRMSHtLCtGDlx/gQ9I3uyLn4d6x9Z75UE/9JLzsO069aFatW8EKSIeJV68sXE\njiM76BXZi671u/Jm1zcpFXL5T+A4dQpiY7Nu27wZ3nrLKVsgIoFHSb4Y+H7P9wycM5BXO7/KsBbD\nCnSMtDTo2ROOH4eKFbPua97c6c2LSOBRkg9y02KmMe67cXx+3+d0rNexwMd5911ISYGffoIQz57y\nJyIBQEk+SKXZNMZ9N4452+ewfOjyAt3glNnSpfDMM0rwIkWNknwQOpd8jgejHiTxdCJrhq8htHxo\nnm2/+srppedn40Z47DEvBikifqEkH2QSTyfSM7InDas3ZMngJZQpWeaS7detgxo14P77L33cEiVy\nliwQkcCnJB9EthzaQo/PejD05qGMv2M8JpfZ0N274aabnPF1gORkePNN6NbNz8GKiF8oyQeJb3/9\nlgfmPsDb4W8z6MZBubY5dw5Wr4brr4e1ay9uL1vWT0GKiN/pZqgg8H8//h8PRT3El/2/zDPBA3zy\nCfz1r9CxI5Qrd/FLyx9Fgpdq1xRhqWmpPLfkORbuXMjCQQupX60+33wDW7bk3n7VKqhZE957z79x\nikjhFKZ2jZJ8EXUm6Qz3z72fkxdOMqffHKqWqwpA+/ZOiYFa2Z/dla5HD6cnLyJFhwqUFTMJpxLo\n+VlPmtdozud9P6d0SGkAhgxxHrn3z39C27buxigigUE9+SJm08FN9PisByNbjmTs7WOzrKAxxln3\n3rUrlLr80jQiEqA0XFNMLNq5iCFRQ3jv7vfo26wvAJ9+Ck884exPSYHTp10MUER8wqdPhpLAMGn9\nJB6e/zDzB87PSPDHjjm1ZB54AP7zHzh0yN0YRSTwaEw+wKWmpfL04qf5bs93rBq2iuuqXpexb+RI\nZ737hAlQpYp7MYpI4FKSD2CnLpxi4JyBJKUmsWrYKqqUvZjJo6IgLs6p5X7vvS4GKSIBTcM1AWr/\nyf10+KgDtSrWYuGghVkSPDjFwlq2hFtvdSlAESkSNPEagH5K+Ilekb14qu1TPNPumYwVNFu2wMsv\ng7Uwfz7s3esUFxOR4KaJ1yAyL24e3T/tzrvd3+XZ9s9mWSIZGwsJCXDfffDFF3DVVS4GKiJFgsbk\nA4S1lrfXvs0ba95g4aCFtKrVKsv+iRPh73+Hfv2cLxERTyjJB4CUtBRGLxrNqn2rWDN8DXUq18my\n/+hRpxc/dqxTYExExFNK8i47eeEk/b7ohzGGlcNWUqlMpRxtuneHxER48EEoc+lngIiIZKExeRft\n/X0vt027jfpV67Ng4IJcEzxAUhLMmwfh4X4OUESKPCV5l6yPX0/7ae15uMXDTLp7EiVL6I8qEfE+\nZRYXzNk2h8cWPsa0ntPo0ahHjv3Ll8OHH158v3evH4MTkaCiJO9H1lpeX/06765/l28f+JYW17TI\ntV10NJw4AX36OO+7d4emTf0Xp4gEDyV5P0lOTWbkwpFsPLCRtcPXUqtSHk/1SHfTTc5Eq4hIYSjJ\n+8Hv53+nz+d9qFCqAsuHLueK0ldw4oTTW8+1/e9wxRX+jVFEgpOSvI/tPr6be2beQ3j9cN7o+gYh\nJUIA6NABDh/O++EeL77oxyBFJGgpyfvQ6n2r6fN5H8bfMZ7HWz0OOHVn5s51ar//8AM0auRykCIS\n1JTkfSRySyR/+fovTI+YTvcG3TO2HzsGAwfC4MFQs6aLAYpIsaAqlF5mreXvK/7OBxs/YMHABTSv\n0TzL/qNHoWFD518REU8UpgqlevJedCHlAo9+9SjbDm9j7fC1XFPxGrdDEpFizqM7Xo0x4caYOGPM\nL8aYMZdo18oYk2yMKXbPKjp27hhdZ3Tl1IVTRD8UnWuCj4mBdu2gdGkXAhSRYinfJG+MKQFMAroB\nzYCBxpjGebT7B7DY20EGup1Hd9Juajva1GrD7H6zqVC6Qo42KSnw889Qq5aT7EVE/MGTnnxrYKe1\ndq+1NhmIBHrl0m40MBs45MX4At6KvSvo8FEHnm33LBPvmkgJk/slfeUVePxxaN4crr7az0GKSLHl\nSZKvBezL9H5/+rYMxpiaQIS19l9AgSYHiqIZm2fQ5/M+/Lv3v3nk1kfybLdpE2zcCP/93/C//+vH\nAEWk2PPWxOvbQOax+qBO9NZaJkRP4JPNn7DsoWU0u6rZJds/+aTzb/v2fghORCQTT5J8PJD5UUW1\n07dl1hKINM4DSUOB7saYZGvt/OwHmzBhQsbrsLAwwsLCLjNkd51POc+wecPYfXw3a4evpcYV+T9J\n21p46SXo2NEPAYpIkRcdHU10dLRXjpXvOnljTAiwA+gMHADWAwOttdvzaP8RsMBaOzeXfUV6nfzh\nM4fpPas3NSvWZHrEdMqVKufR5zp2VJIXkYIrzDr5fMfkrbWpwCjgW2ArEGmt3W6MGWGMeTS3jxQk\nkEAXdySOdlPb0bFuRyLvi/Q4wd91F/z4Y941akREfEl3vHrg+z3fM3DOQP7R+R8MbTH0sj5bqhRs\n2OCsqimh53CJSAEUpievJJ+Pj2I+Yux3Y4nsE0mn6zpd9udLlYKzZ9WTF5GCU1kDH0izaTz//fN8\nvvVzlg9ZTqPQyysXmZwMS5dCWpqPAhQR8YCSfC7OJZ/joaiHSDiVwNqH1xJaPvSyj7FpEwwYAIMG\nQUiID4IUEfGARomzSTydSKfpnSgVUoqlDy4tUIIHZ9lkw4bw739rLF5E3KP0k8nWQ1tpO7Ut4TeE\nM6P3DMqWLOt2SCIihaLhmnRLfl3C/XPv55/d/skDzR8o8HHefBM+/xxOn4ZKlbwYoIhIASjJA1N+\nmsL/W/b/mNNvDh3qdsi1jbWQlJT/sdasgR49nPXx16icvIi4rFgn+TSbxpglY5i3Yx4rhq6gQfUG\nebb9179g9Ggomc8VMwZGjoQ2bbwcrIhIARTbdfJnks7wwJcPcPzcceb2n0u1ctUAiI+HrVtzto+M\nhBo14NVX/RyoiBR7Wid/mQ6cOkCPz3rQ7KpmzLpvFqVDLj6q6eWXYflyqF075+d69vRjkCIiXlDs\nkvzmxM30+KwHj97yKOM6jMMpnOn49FNYtw6eegoeza0qj4hIEVOskvyinYsYEjWEd7q/w4A/Dcix\n/9VXoVMn6NrVheBERHyg2CT5OdvmMOrrUUQNiKL9tXk/vWPECKhXz39xiYj4UrFI8kfPHuWJRU8w\nf+B8Wtdq7XY4IiJ+UyyS/LNLnmXAnwbkmeDPn4fQULhwASpW9HNwIiI+FPRJ/rvd3/H9nu/ZMnJL\nnm2Skpz6MsnJfgxMRMQPgrp2zbnkc4z4agST755MxTLqootI8RPUSf7l5S9za81buafhPW6HIiLi\niqAdrtmcuJkPN37I5pGb3Q5FRMQ1QdmTT01L5ZEFj/BK51e4+oqr3Q5HRMQ1QZnk39vwHmVLlmVY\ni2FuhyIi4qqgG6757cRvvPTDS6watooSJih/h4mIeCyokry1licWPcGTbZ685IO3f/kFfv/94vvT\np/0QnIiIC4Iqyc/eNpvdx3czp9+cS7Zr2RIaNMj6gG3VqxGRYBQ09eSPnztOs8nNmN1v9iVr0wCU\nLw9Hjjj/iogEusLUkw+aQesxS8cQ0Tgi3wQvIlKcBMVwzfK9y1m0cxFbH7/4SKfz5+Ghh+DcuZzt\nL1xwHtMnIhLsivxwzYWUC9z0/k282vlVejfpnbH94EFo3Bg++STnZypWdOrGi4gUBYUZrinySf6F\nZS/w86Gfmdt/bsa2lSvhz3+G6tXh11+9ejoREb8rts943XZ4G5N/nEzsiNgs2w8dgttvh9mzXQpM\nRCRAFNmJ1zSbxiMLHuHFsBepValWjv2lS0PZsi4EJiISQIpskp/y0xQAHmv5mMuRiIgEriI5XJNw\nKoHxy8YT/VC0SheIiFxCkcyQo78ezciWI2l2VbMc+xYuhNdf1xJJEREogj35qLgothzawqf3fprr\n/mXL4IYbYNw4PwcmIhKAilSSP3nhJKO/Hs2M3jMoWzLvWdWbboImTfwYmIhIgPJouMYYE26MiTPG\n/GKMGZPL/kHGmE3pXyuNMTd6P1QY9904wuuH07Fex1z3HzvmfImIiCPfnrwxpgQwCegMJAAbjDHz\nrLVxmZrtBu6w1p4wxoQDHwBtvRnomn1rmLt9bpbSBdndey/s3g09e3rzzCIiRZcnwzWtgZ3W2r0A\nxphIoBeQkeSttWsztV8L5Fy4XghJqUk8suAR3g5/m6rlqubdLgkiI6G9apSJiACeDdfUAvZler+f\nSyfxh4GvCxNUdq+vep16VerRt2lfbx5WRCToeXXi1RjTCRgK3J5XmwkTJmS8DgsLIyws7JLH/OXo\nL7y19i02jtiIyWVdZGIijBkDqamwc6eWTopI0RcdHU10dLRXjpVvgTJjTFtggrU2PP39WMBaa1/L\n1q45MAcIt9bmWhbscguUWWvpNL0TvRv35sm2T+baZvVqePBBeOEFKFkSevdWOQMRCS6+LlC2AbjB\nGFMXOAAMAAZmC6AOToIfnFeCL4iPYj/ibPJZRrUelWPf3/4GM2c6deObNoXBg711VhGR4JFvkrfW\nphpjRgHCD4FsAAAIA0lEQVTf4ozhT7XWbjfGjHB22ynAeKAaMNk4YyrJ1trWhQks8XQiY5eOZcng\nJYSUCMmy79gxiIlxEn337lCtWmHOJCISvAK2nvzAOQOpW7ku/+jyjxz76taFtDSnlHCbNt6OUkQk\nsARdPflFOxexIX4DU3tOzdh2/LhTl8Za+P132LEDrr7axSBFRIqAgEvyp5NOM3LhSKb2nEr5UuUz\nti9aBOPHOw8D6d9fQzQiIp4IuOGavy7+K0fPHWV6xPSMbd98A6+9BjVrwqe51yUTEQlaQTNcsyF+\nAzN/nsmWx7dk2b5gAVxzDTz3nEuBiYgUUQFTTz45NZlHFjzCG13fILR8aI79t93mVJcUERHPBUyS\nf2vtW1xV4Sruv/F+t0MREQkaATFccz7lPK+seCXP0gUiIlIwAdGTX/nbSppd1Yzrq16fY9+GDbBr\nlwtBiYgEgYBI8t/s+oZu9bvlum/ECOfGp9aFun9WRKR4Cvgkby1MnAitWvk5KBGRIOB6kt9/cj8H\nTx+kZc2WbociIhJ0XE/yi3ctpsv1XXIUIRMRkcJzP8n/upjwG8LdDkNEJCi5muRT0lJYunspXet3\ndTMMEZGg5WqS3xC/gdqValOzYs1c9y9bBidO+DkoEZEg4mqSX753OXded2eu+86fhy5doEULqF3b\nz4GJiAQJV+94jTkYw90N7s54P3eu88xWgORkKF0a5sxxKTgRkSDgak8+5mAMLa5ukfH+gw+cB4Jc\nfTVcey1MmeJicCIiQcC1nvzppNPsO7GPxqGNM7ZduAB9+0K33O+LEhGRy+RaT35z4maaXtmUUiGl\nAGeC9aefVL5ARMSbXEvyMQeyDtVERUFYGFSt6lZEIiLBx7UkH3swlhbXXEzyM2fCoEFuRSMiEpzc\n68lnm3Rdtw46d3YrGhGR4OTKxGtyajLbDm/jxho3cvYsvP46nDsHpUq5EY2ISPBypScfdySOOpXr\ncEXpK9izB957D157DSpVciMaEZHgZay1/juZMdZay13PfkLsma9pvOUzzpyBlBTYvNlvYYiIFCnG\nGKy1BXo2qivDNT/GxxB+WwseT59oveYaN6IQEQl+riT5C1Vj6PKn7nTo4MbZRUSKD7+PyVtruVA1\nlqZVW+TfWERECsXvSf4/v/+HEilXUK3slf4+tYhIseP3JP/ilBhMonrxIiL+4PckP+O7WNpfdzN1\n6/r7zCIixY/fk3yJmjH8pV8Lypb195lFRIofvyf5qk2yljMQERHf8XuST+IM9arU8/dpRUSKJY+S\nvDEm3BgTZ4z5xRgzJo827xhjdhpjYo0xN+d1rHa122FMgW7cEhGRy5RvkjfGlAAmAd2AZsBAY0zj\nbG26A/WttQ2AEcD7eR3vtmtvK1TAwSI6OtrtEAKGrsVFuhYX6Vp4hyc9+dbATmvtXmttMhAJ9MrW\nphfwCYC1dh1Q2RhTI7eD3V7n9kKEGzz0DXyRrsVFuhYX6Vp4hydJvhawL9P7/enbLtUmPpc2ALSq\n1epy4hMRkULw+8Rr+VLl/X1KEZFiK99Sw8aYtsAEa214+vuxgLXWvpapzfvAMmvtrPT3cUBHa21i\ntmP5r66xiEgQ8WWp4Q3ADcaYusABYAAwMFub+cATwKz0Xwq/Z0/whQlSREQKJt8kb61NNcaMAr7F\nGd6Zaq3dbowZ4ey2U6y1i4wxdxtjdgFngKG+DVtERDzh1ydDiYiIf/lk4tWbN08VdfldC2PMIGPM\npvSvlcaYG92I0x88+b5Ib9fKGJNsjLnXn/H5k4c/I2HGmBhjzBZjzDJ/x+gvHvyMVDLGzE/PFT8b\nY4a4EKbPGWOmGmMSjTF5Pgy1QHnTWuvVL5xfHLuAukApIBZonK1Nd2Bh+us2wFpvxxEIXx5ei7ZA\n5fTX4cX5WmRq9x3wFXCv23G7+H1RGdgK1Ep/H+p23C5ei78Br/5xHYCjQEm3Y/fBtbgduBnYnMf+\nAuVNX/TkvXrzVBGX77Ww1q611p5If7uWPO4vCAKefF8AjAZmA4f8GZyfeXItBgFzrLXxANbaI36O\n0V88uRYWqJj+uiJw1Fqb4scY/cJauxI4fokmBcqbvkjyXr15qojz5Fpk9jDwtU8jck++18IYUxOI\nsNb+CwjmlViefF80BKoZY5YZYzYYYwb7LTr/8uRaTAKaGmMSgE3Ak36KLdAUKG+68iBvyckY0wln\nVVJxrvvwNpB5TDaYE31+SgK3AHcCFYA1xpg11tpd7oblim5AjLX2TmNMfWCJMaa5tfa024EVBb5I\n8vFAnUzva6dvy97m2nzaBANPrgXGmObAFCDcWnupP9eKMk+uRUsg0jhlSkOB7saYZGvtfD/F6C+e\nXIv9wBFr7XngvDFmOXATzvh1MPHkWgwFXgWw1v5qjNkDNAZ+9EuEgaNAedMXwzUZN08ZY0rj3DyV\n/Yd0PvAgZNxRm+vNU0Eg32thjKkDzAEGW2t/dSFGf8n3Wlhrr0//ug5nXP7xIEzw4NnPyDzgdmNM\niDGmPM5E23Y/x+kPnlyLvUAXgPQx6IbAbr9G6T+GvP+CLVDe9HpP3urmqQyeXAtgPFANmJzeg022\n1rZ2L2rf8PBaZPmI34P0Ew9/RuKMMYuBzUAqMMVau83FsH3Cw++L/wE+zrS08Dlr7TGXQvYZY8xM\nIAyoboz5DXgBKE0h86ZuhhIRCWJ+r0IpIiL+oyQvIhLElORFRIKYkryISBBTkhcRCWJK8iIiQUxJ\nXkQkiCnJi4gEsf8Pzf5wCe+YChYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11481b150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a ROC curve for both classifiers on the same plot\n",
    "# does this match expectations from the AUC values?\n",
    "fpr_lr, tpr_lr, thresh_lr= roc_curve(y, prob_lr)\n",
    "fpr_knn, tpr_knn, thresh_knn= roc_curve(y, prob_knn)\n",
    "\n",
    "# add code here to plot roc curve\n",
    "plt.figure()\n",
    "ax= plt.plot(fpr_lr, tpr_lr) #blue line\n",
    "ax= plt.plot(fpr_knn, tpr_knn) #green line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is one model performing so much better than the other?\n",
    "#### What is likely happening?\n",
    "\n",
    "Answer: When running both clasifiers on the same training data, it seems that KNN performs significantly better. This is likely due to that estimator overfitting the available data- exacerbated by the fact that we are training and testing on the same set. \n",
    "\n",
    "To get a clearer answer on which model is performing better, it would be wiser to split the data into test and train sets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a train/test split of the data\n",
    "# Retrain both models using the training set\n",
    "# How do the AUC values change when only using P(y=1|x) on the test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>prestige</th>\n",
       "      <th>prestige_1</th>\n",
       "      <th>prestige_2</th>\n",
       "      <th>prestige_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.00000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.317500</td>\n",
       "      <td>588.040000</td>\n",
       "      <td>3.390930</td>\n",
       "      <td>2.48500</td>\n",
       "      <td>0.152500</td>\n",
       "      <td>0.377500</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.466087</td>\n",
       "      <td>115.338353</td>\n",
       "      <td>0.379675</td>\n",
       "      <td>0.94446</td>\n",
       "      <td>0.359955</td>\n",
       "      <td>0.485369</td>\n",
       "      <td>0.459916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>520.000000</td>\n",
       "      <td>3.130000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>580.000000</td>\n",
       "      <td>3.390930</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>660.000000</td>\n",
       "      <td>3.670000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            admit         gre         gpa   prestige  prestige_1  prestige_2  \\\n",
       "count  400.000000  400.000000  400.000000  400.00000  400.000000  400.000000   \n",
       "mean     0.317500  588.040000    3.390930    2.48500    0.152500    0.377500   \n",
       "std      0.466087  115.338353    0.379675    0.94446    0.359955    0.485369   \n",
       "min      0.000000  220.000000    2.260000    1.00000    0.000000    0.000000   \n",
       "25%      0.000000  520.000000    3.130000    2.00000    0.000000    0.000000   \n",
       "50%      0.000000  580.000000    3.390930    2.00000    0.000000    0.000000   \n",
       "75%      1.000000  660.000000    3.670000    3.00000    0.000000    1.000000   \n",
       "max      1.000000  800.000000    4.000000    4.00000    1.000000    1.000000   \n",
       "\n",
       "       prestige_3  \n",
       "count  400.000000  \n",
       "mean     0.302500  \n",
       "std      0.459916  \n",
       "min      0.000000  \n",
       "25%      0.000000  \n",
       "50%      0.000000  \n",
       "75%      1.000000  \n",
       "max      1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=df[:300]\n",
    "test= df[300:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train[['gre','gpa','prestige_1','prestige_2','prestige_3']]\n",
    "y_train = train.admit\n",
    "X_test= test[['gre','gpa','prestige_1','prestige_2','prestige_3']]\n",
    "y_test= test.admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=13, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrain both classifiers (logistic regression and kNN using best parameter found) on the full dataset\n",
    "lr =LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "knn= KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob_lr= lr.predict_proba(X_test)[:,1]\n",
    "prob_knn= knn.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573626373626\n",
      "0.567032967033\n"
     ]
    }
   ],
   "source": [
    "# print AUC for both classifiers\n",
    "print metrics.roc_auc_score(y_test, prob_lr)\n",
    "\n",
    "print metrics.roc_auc_score(y_test, prob_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114e750d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr_lr, tpr_lr, thresh_lr= roc_curve(y_test, prob_lr)\n",
    "fpr_knn, tpr_knn, thresh_knn= roc_curve(y_test, prob_knn)\n",
    "\n",
    "# add code here to plot roc curve\n",
    "plt.figure()\n",
    "ax=  plt.plot(fpr_lr, tpr_lr) #blue line\n",
    "ax= plt.plot(fpr_knn, tpr_knn) #green line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it seems the logistical regression model is working slightly better. This time, the AUC scores for the two models are much less distant from eachother than they were earlier when we testing on our training data set. This indicates we may have gotten rid of the overfitting, or whatever else may have been going on earlier. \n",
    "\n",
    "\n",
    "We are still seeing the same chatacteristics of having\n",
    "a jagged line for the logistic regression ROC line, and a much smoother one for the KNN ROC plot. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
